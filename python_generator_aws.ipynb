{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9SN8/jzKwdJew/H5aUREm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlessandraMayumi/python-generator/blob/main/python_generator_aws.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "When developing a solution, start with the most simple thinking, nice controlled scenarios then increment the implementation aganist corner cases. Even being creative, it's more common than not that the production usage is even more creative, stressing resources and requiring new ideas.\n",
        "\n",
        "I want to share how to develop a implementation from the high level description of a problem to the solution increasing complexity as the new cases happen."
      ],
      "metadata": {
        "id": "D0RMFPEvaf_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem\n",
        "Generate a response containing how many empty cells for each column in a csv, the file is stored in a s3 bucket. This process have to be in a aws lambda."
      ],
      "metadata": {
        "id": "CmH1cDDZZ5KM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hands on\n"
      ],
      "metadata": {
        "id": "WT37KiyqbLzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a project\n",
        "I started this python study project in *GitHub* repository https://github.com/AlessandraMayumi/python-generator and to save this Colab notebook."
      ],
      "metadata": {
        "id": "IPdGXRkKT-H1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading a csv file with pandas\n",
        "Before thinking about aws resources, let's try to retrieve the result from a local csv file as expected in the problem description.\n",
        "\n",
        "#### References\n",
        "- w3schools: https://www.w3schools.com/python/pandas/pandas_csv.asp\n",
        "\n"
      ],
      "metadata": {
        "id": "Gdd9NJtJSThq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56GcCDn7IhID",
        "outputId": "8cb12fe0-477f-4a07-d2e0-98bb14f4d20c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
            "0    -122.05     37.37                27.0       3885.0           661.0   \n",
            "1    -118.30     34.26                43.0       1510.0           310.0   \n",
            "2    -117.81     33.78                27.0       3589.0           507.0   \n",
            "3    -118.36     33.82                28.0         67.0            15.0   \n",
            "4    -119.67     36.33                19.0       1241.0           244.0   \n",
            "\n",
            "   population  households  median_income  median_house_value  \n",
            "0      1537.0       606.0         6.6085            344700.0  \n",
            "1       809.0       277.0         3.5990            176500.0  \n",
            "2      1484.0       495.0         5.7934            270500.0  \n",
            "3        49.0        11.0         6.1359            330000.0  \n",
            "4       850.0       237.0         2.9375             81700.0  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/sample_data/california_housing_test.csv')\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare a test csv file\n",
        "Occasionally generating the test environment can take more time then implementing the initial ideas for the solution.\n",
        "\n",
        "In this case, I used a website that generates csv file, but what I wanted to validate was empty cells, so created a script do clear some values.\n",
        "\n",
        "#### References\n",
        "- python open - https://www.pythontutorial.net/python-basics/python-read-text-file/\n",
        "- csv generator: https://extendsclass.com/csv-generator.html"
      ],
      "metadata": {
        "id": "KFSwGbzjl2BR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "\"\"\" The test csv file should have some empty cells \"\"\"\n",
        "\n",
        "filename_test = 'mock_empty_register.csv'\n",
        "filepath_test = f'/content/sample_data/{filename_test}'\n",
        "filepath_original = '/content/sample_data/mock_register.csv'\n",
        "\n",
        "def modify_empty_cells():\n",
        "  df = pd.read_csv(filepath_original)\n",
        "\n",
        "  num_row, num_col = df.shape\n",
        "\n",
        "  for i in range(100):\n",
        "    x = random.randrange(num_row)\n",
        "    y = random.randrange(1, num_col)\n",
        "    df.loc[x, df.columns[y]] = None\n",
        "\n",
        "  df.to_csv(filepath_test)\n",
        "\n",
        "  print(f'Test csv file generated: {filename_test}')\n",
        "\n",
        "\n",
        "modify_empty_cells()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKmkYB2ol0q2",
        "outputId": "e91b007f-587f-4df6-8078-b1abb20cbfff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test csv file generated: mock_empty_register.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working on the solution\n",
        "After the test file is generated, let's work on the solution.\n",
        "\n",
        "## Count empty cells\n",
        "Pandas already have a easy way to do it.\n",
        "\n",
        "## Convert dataframe into dict"
      ],
      "metadata": {
        "id": "6QroJ8vPFkcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv(filepath_test)\n",
        "df_test.isna().sum().to_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryA2jGBpGpW2",
        "outputId": "819025ae-9e02-4a23-bbb1-885d9c98d839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucessfully count empty cells for each column: {'firstname': 22, 'lastname': 18, 'email': 21, 'email2': 22, 'profession': 17}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a aws lambda\n",
        "Create a aws lambda function trigged by S3 bucket, so everytime a file is uploaded, the lambda would run.\n",
        "\n",
        "#### References\n",
        "- aws lambda: https://docs.aws.amazon.com/lambda/latest/dg/getting-started.html"
      ],
      "metadata": {
        "id": "SxJ27AEiu794"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    print(f'Lambda event: {event}')\n",
        "\n",
        "    return {\n",
        "        'statusCode': 200,\n",
        "        'body': json.dumps('Hello from Lambda!')\n",
        "    }\n"
      ],
      "metadata": {
        "id": "sHkWeVC4vqnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add a Test\n",
        "Instead of adding and removing the same csv file, print the lambda event in order to learn how the test can mock a new file in S3 bucket\n",
        "\n",
        "Event Json is something like this:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"Records\": [\n",
        "    {\n",
        "      \"eventVersion\": \"2.1\",\n",
        "      \"eventSource\": \"aws:s3\",\n",
        "      \"awsRegion\": \"us-east-1\",\n",
        "      \"eventTime\": \"2023-07-02T19:25:52.815Z\",\n",
        "      \"eventName\": \"ObjectCreated:Put\",\n",
        "      \"userIdentity\": {\n",
        "        \"principalId\": \"A25Y...\"\n",
        "      },\n",
        "      \"requestParameters\": {\n",
        "        \"sourceIPAddress\": \"187.106.35.106\"\n",
        "      },\n",
        "      \"responseElements\": {\n",
        "        \"x-amz-request-id\": \"XKK...\",\n",
        "        \"x-amz-id-2\": \"PAT...\"\n",
        "      },\n",
        "      \"s3\": {\n",
        "        \"s3SchemaVersion\": \"1.0\",\n",
        "        \"configurationId\": \"7331....\",\n",
        "        \"bucket\": {\n",
        "          \"name\": \"my-bucket\",\n",
        "          \"ownerIdentity\": {\n",
        "            \"principalId\": \"A25...\"\n",
        "          },\n",
        "          \"arn\": \"arn:aws:s3:::my-bucket\"\n",
        "        },\n",
        "        \"object\": {\n",
        "          \"key\": \"mock_empty_register.csv\",\n",
        "          \"size\": 81685,\n",
        "          \"eTag\": \"6fc5f1...\",\n",
        "          \"sequencer\": \"0064...\"\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "A2EYkMMri3oG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add permission to read S3 object\n",
        "\n",
        "Before accessing the S3 file, is necessary to configure the lambda to permit the `get object` operation.\n",
        "\n",
        "Navigate through: *Monitor > Permissions*.\n",
        "Then click on the lambda role and edit.\n",
        "In the role page, there is a Permissions policies section, add the permission: **AmazonS3ReadOnlyAccess**\n"
      ],
      "metadata": {
        "id": "Lpae50d6wZGg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get object from S3 bucket and read as csv\n",
        "\n",
        "Boto3 is the Amazon Web Services (AWS) Software Development Kit (SDK) for Python. To retrieve objects from S3 it is necessary to parse the handler event which contains the bucket name and object key. The response type for body is StreamingBody that requires a conversion for pandas be able to read it.\n",
        "\n",
        "#### References\n",
        "- aws sdk boto3 for python: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/get_object.html"
      ],
      "metadata": {
        "id": "KsvPT-6Zu7Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    print(f'Lambda event: {event}')\n",
        "\n",
        "    bucket = event['Records'][0]['s3']['bucket']['name']\n",
        "    object_key = event['Records'][0]['s3']['object']['key']\n",
        "\n",
        "    client = boto3.client('s3')\n",
        "    obj = client.get_object(Bucket=bucket, Key=object_key)\n",
        "    body_bytes = obj['Body'].read()\n",
        "    body_io = io.BytesIO(body_bytes)\n",
        "\n",
        "    df = pd.read_csv(body_io)\n",
        "    empty_count = df.isna().sum().to_json()\n",
        "    print(f'Sucessfully count empty cells for each column: {empty_count}')\n",
        "\n",
        "    return {\n",
        "        'statusCode': 200,\n",
        "        'body': [empty_count]\n",
        "    }"
      ],
      "metadata": {
        "id": "imD73bi2fBmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Big files aganist lambda memory\n",
        "\n",
        "AWS Lambdas standard memory is *128 MB*, so if the csv file is 100Mb the lambda cannot read the file entirely and then evaluete it.\n",
        "\n",
        "#### References\n",
        "- aws lambda memory: https://docs.aws.amazon.com/lambda/latest/operatorguide/computing-power.html"
      ],
      "metadata": {
        "id": "TyADho_v0ob9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create file tests using a script\n",
        "Instead of using sites to generate new files, let's create a script in order to have more control over test files."
      ],
      "metadata": {
        "id": "f_TY9kjxndbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" CSV FILE GENERATOR\n",
        "To generate test csv files within empty cells, call python functions\n",
        "\n",
        "generate_csv(lines)\n",
        "modify_empty_cells()\n",
        "\"\"\"\n",
        "import uuid\n",
        "import random\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "FILENAME = '../data/mock_big_register.csv'\n",
        "\n",
        "\n",
        "def _add_headers():\n",
        "    headers = \"id,firstname,lastname,email,email2,phone,profession\\n\"\n",
        "    empty_file = open(FILENAME, 'w')\n",
        "    empty_file.write(headers)\n",
        "    empty_file.close()\n",
        "\n",
        "\n",
        "def _get_occupations():\n",
        "    occupations = []\n",
        "    with open('occupations.csv') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        for row in reader:\n",
        "            occupations.append(row['Occupations'])\n",
        "    return occupations\n",
        "\n",
        "\n",
        "def _generate_line(professions):\n",
        "    name = f'name{random.randrange(1000)}'\n",
        "    surname = f'surname{random.randrange(1000)}'\n",
        "    email = f'{name}@email.com'\n",
        "    email2 = f'{name}@email2.com'\n",
        "    phone = random.randrange(100000, 999999)\n",
        "    profession = professions[random.randrange(len(professions))]\n",
        "    return f'{uuid.uuid4()}, {name}, {surname}, {email}, {email2}, {phone}, {profession}\\n'\n",
        "\n",
        "\n",
        "def generate_csv(lines):\n",
        "    \"\"\"\n",
        "    Generate a test csv file specifying how many lines\n",
        "    :param lines: quantity of lines the csv should have\n",
        "    \"\"\"\n",
        "    _add_headers()\n",
        "    occupation_list = _get_occupations()\n",
        "\n",
        "    with open(FILENAME, 'a') as f:\n",
        "        for i in range(lines):\n",
        "            line = _generate_line(occupation_list)\n",
        "            f.write(line)\n",
        "\n",
        "    print('Successfully generated')\n",
        "\n",
        "\n",
        "def modify_empty_cells():\n",
        "    df = pd.read_csv(FILENAME)\n",
        "\n",
        "    num_row, num_col = df.shape\n",
        "\n",
        "    for i in range(100):\n",
        "        x = random.randrange(num_row)\n",
        "        y = random.randrange(2, num_col)\n",
        "        df.loc[x, df.columns[y]] = None\n",
        "\n",
        "    empty = df.isna().sum()\n",
        "    print('Checking empty cells', empty)  # .to_csv()\n",
        "    df.to_csv(FILENAME)\n",
        "\n",
        "    print(f'Test csv file generated: {FILENAME}')\n",
        "\n",
        "\n",
        "generate_csv(100000)\n",
        "modify_empty_cells()\n"
      ],
      "metadata": {
        "id": "2rnRpJlRmMkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iterate lines instead of reading the entire file body\n",
        "\n",
        "Sometimes a solution change completely due to resources limitations. In order to read a larger file using the same memory limitation, let's read each line and evaluate separately. The side effect is the ingrease of the duration.\n",
        "\n",
        "In this case, the implementation started with dataframes only because is a simple ready to use operation. However, the iteration of lines does not enable a direct dataframe creation.\n",
        "\n",
        "This new solution is parsing each line.\n",
        "\n",
        "#### References\n",
        "- boto3: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/get_object.html"
      ],
      "metadata": {
        "id": "UT9Q5AJGkFjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lambda_handler(event, context):\n",
        "    bucket = event['Records'][0]['s3']['bucket']['name']\n",
        "    object_key = event['Records'][0]['s3']['object']['key']\n",
        "\n",
        "    client = boto3.client('s3')\n",
        "\n",
        "    obj = client.get_object(Bucket=bucket, Key=object_key)\n",
        "    generator = obj['Body'].iter_lines()\n",
        "\n",
        "    headers = next(generator).decode('utf-8').split(',')\n",
        "    # initialize dictionary\n",
        "    empty_count = {}\n",
        "    for h in headers:\n",
        "        empty_count[h] = 0\n",
        "    # count empty cells\n",
        "    for line in generator:\n",
        "        row = line.decode('utf-8').split(',')\n",
        "        for idx, value in enumerate(row):\n",
        "            if not value:\n",
        "                empty_count[headers[idx]] += 1\n",
        "\n",
        "    print(f'Successfully count empty cells for each column: {empty_count}')\n",
        "\n",
        "    return {\n",
        "        'statusCode': 200,\n",
        "        'body': [empty_count]\n",
        "    }"
      ],
      "metadata": {
        "id": "n33Xkqaw4Bl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using dataframes in chunks\n",
        "\n",
        "What if it was the case of a real difference using dataframes instead of evaluating each csv line.\n",
        "\n"
      ],
      "metadata": {
        "id": "21J3HeGWo8LR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_csv_chunk(stream_body):\n",
        "    columns = []\n",
        "    pending = ''\n",
        "    # increase chunk size to optimize duration while keeping low memory usage\n",
        "    for chunk in stream_body.iter_chunks(chunk_size=1000000):\n",
        "        decoded = pending + chunk.decode('utf-8')\n",
        "        rows_list_raw = decoded.splitlines()\n",
        "        # the splitlines function do not keep the newline character '\\n',\n",
        "        # so if the last caracter in the chunk is a '\\n' this caracter is lost\n",
        "        # and the next iteraction will merge two lines\n",
        "        if decoded[-1] != '\\n':\n",
        "            pending = rows_list_raw[-1]\n",
        "            rows_list = rows_list_raw[0:-1]\n",
        "        else:\n",
        "            pending = ''\n",
        "            rows_list = rows_list_raw\n",
        "        # to create each chunk dataframe, it needs column names\n",
        "        # that are present only in the first chunk\n",
        "        # so store a list of column names and use in each iteration\n",
        "        if not columns:\n",
        "            columns = rows_list[0].split(',')\n",
        "            rows_list = rows_list[1::]\n",
        "        yield [row.split(',') for row in rows_list], columns\n",
        "\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    bucket = event['Records'][0]['s3']['bucket']['name']\n",
        "    object_key = event['Records'][0]['s3']['object']['key']\n",
        "\n",
        "    client = boto3.client('s3')\n",
        "\n",
        "    obj = client.get_object(Bucket=bucket, Key=object_key)\n",
        "    stream_body = obj['Body']\n",
        "\n",
        "    empty_count = None\n",
        "    for rows, columns in _get_csv_chunk(stream_body):\n",
        "        df = pd.DataFrame(rows, columns=columns).replace('', None)\n",
        "\n",
        "        empty_count_chunk = df.isna().sum().to_dict()\n",
        "        if not empty_count:\n",
        "            empty_count = empty_count_chunk\n",
        "        for key in empty_count_chunk:\n",
        "            empty_count[key] += empty_count_chunk[key]\n",
        "\n",
        "    print(f'Successfully count empty cells for each column: {empty_count}')\n",
        "\n",
        "    return {\n",
        "        'statusCode': 200,\n",
        "        'body': [empty_count]\n",
        "    }"
      ],
      "metadata": {
        "id": "N2_mL7Y-miuu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}